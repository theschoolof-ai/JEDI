{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"EVA5 - Session 5_3.ipynb","provenance":[{"file_id":"1-c8WXOtg5RGpl8DmkZSD9HFsXOCIWuy2","timestamp":1597687573540},{"file_id":"https://github.com/realpranav93/EVA5/blob/master/S4/EVA5_Session_4_pranav.ipynb","timestamp":1597680763576}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"YeNnKnAZ3Brn","colab_type":"text"},"source":["###Target:\n","Reach 99.4% accuracy with the power of image agumentation. Try to find the better Batch size for Mnist and this Architecture in 32, 64, 128, 256.\n","\n","###Results:\n","1. Parameters: 7,917\n","2. Best Train Accuracy: 99.24%\n","3. Best Test Accuracy: 99.48%\n","\n","###Analysis:\n","1. Image agumentation works as good regularization\n","2. 64 batch size showcases better performance than 32, 64, 128, 256\n","3. Model takes more time to converge with LR - 0.01 and was able to reaach > 99.4% accuracy only once in 14th epoch.\n","4. Adding more regularisation can make the model robust and perform consistently on test data.\n","\n"," "]},{"cell_type":"code","metadata":{"id":"0m2JWFliFfKT","colab_type":"code","colab":{}},"source":["from __future__ import print_function\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h_Cx9q2QFgM7","colab_type":"code","colab":{}},"source":["class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","\n","        #block1\n","        self.conv1 = nn.Conv2d(1, 8, 3,padding = True) #28\n","        self.Batchnorm1 = nn.BatchNorm2d(8)\n","        self.conv2 = nn.Conv2d(8, 15, 3) #26\n","        self.Batchnorm2 = nn.BatchNorm2d(15)\n","\n","        #transition block\n","        self.pool1 = nn.MaxPool2d(2, 2) #13\n","        self.pool1trns = nn.Conv2d(15, 10, 1)#13\n","        self.Batchnormtrns1 = nn.BatchNorm2d(10)\n","        \n","        #block2\n","        self.conv3 = nn.Conv2d(10,14,3) #11\n","        self.Batchnorm3 = nn.BatchNorm2d(14)\n","        self.conv4 = nn.Conv2d(14, 16, 3)#9\n","        self.Batchnorm4 = nn.BatchNorm2d(16)\n","        self.conv5 = nn.Conv2d(16, 20, 3)#7\n","        self.Batchnorm5 = nn.BatchNorm2d(20)\n","\n","\n","        #block3\n","        #self.pool2trns = nn.Conv2d(18,10,1) #6\n","        #self.Batchnormtrns2 = nn.BatchNorm2d(10)\n","        #self.conv6 = nn.Conv2d(10,10,7) #1\n","\n","        self.conv6_avgp = nn.AvgPool2d(kernel_size=7)\n","\n","        self.pool2trns = nn.Conv2d(20,10,1) #6\n","        #self.Batchnormtrns2 = nn.BatchNorm2d(10)\n","\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = F.relu(x)\n","        x = self.Batchnorm1(x)\n","\n","        x = self.conv2(x)\n","        x = F.relu(x)\n","        x = self.Batchnorm2(x)\n","\n","\n","        x = self.pool1(x)\n","        x = self.pool1trns(x)\n","        x = F.relu(x)\n","        x = self.Batchnormtrns1(x)\n","\n","        x = self.conv3(x)\n","        x = F.relu(x)\n","        x = self.Batchnorm3(x)\n","\n"," \n","        x = self.conv4(x)\n","        x = F.relu(x)\n","        x = self.Batchnorm4(x)\n","\n","\n","        x = self.conv5(x)\n","        x = F.relu(x)\n","        x = self.Batchnorm5(x)\n","\n","\n","        x = self.conv6_avgp(x)\n","\n","        x = self.pool2trns(x)\n","        #x = F.relu(x)\n","        #x = self.Batchnormtrns2(x)\n","\n","        x = x.view(-1, 10)\n","        return F.log_softmax(x,dim=-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xdydjYTZFyi3","colab":{"base_uri":"https://localhost:8080/","height":485},"executionInfo":{"status":"ok","timestamp":1597861206893,"user_tz":-330,"elapsed":978,"user":{"displayName":"pranav sai","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUzfhrgEZs5GkRg3NiqbkWRHjeSfpEybcsDjhH=s64","userId":"17149605509345190981"}},"outputId":"06da349e-43bf-4232-bbd5-ec44076816b1"},"source":["#!pip install torchsummary\n","from torchsummary import summary\n","use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","model = Net().to(device)\n","summary(model, input_size=(1, 28, 28))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1            [-1, 8, 28, 28]              80\n","       BatchNorm2d-2            [-1, 8, 28, 28]              16\n","            Conv2d-3           [-1, 15, 26, 26]           1,095\n","       BatchNorm2d-4           [-1, 15, 26, 26]              30\n","         MaxPool2d-5           [-1, 15, 13, 13]               0\n","            Conv2d-6           [-1, 10, 13, 13]             160\n","       BatchNorm2d-7           [-1, 10, 13, 13]              20\n","            Conv2d-8           [-1, 14, 11, 11]           1,274\n","       BatchNorm2d-9           [-1, 14, 11, 11]              28\n","           Conv2d-10             [-1, 16, 9, 9]           2,032\n","      BatchNorm2d-11             [-1, 16, 9, 9]              32\n","           Conv2d-12             [-1, 20, 7, 7]           2,900\n","      BatchNorm2d-13             [-1, 20, 7, 7]              40\n","        AvgPool2d-14             [-1, 20, 1, 1]               0\n","           Conv2d-15             [-1, 10, 1, 1]             210\n","================================================================\n","Total params: 7,917\n","Trainable params: 7,917\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.36\n","Params size (MB): 0.03\n","Estimated Total Size (MB): 0.39\n","----------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DqTWLaM5GHgH","colab_type":"code","colab":{}},"source":["torch.manual_seed(1)\n","batch_size = 64\n","\n","kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n","train_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('../data', train=True, download=True,\n","                    transform=transforms.Compose([\n","                        transforms.RandomRotation((-8.0, 8.0), fill=(1,)),\n","                        transforms.ToTensor(),\n","                        transforms.Normalize((0.1307,), (0.3081,))\n","                    ])),\n","    batch_size=batch_size, shuffle=True, **kwargs)\n","\n","\n","test_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('../data', train=False, transform=transforms.Compose([\n","                        transforms.ToTensor(),\n","                        transforms.Normalize((0.1307,), (0.3081,))\n","                    ])),\n","    batch_size=batch_size, shuffle=True, **kwargs)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8fDefDhaFlwH","colab_type":"code","cellView":"both","colab":{}},"source":["from tqdm import tqdm\n","def train(model, device, train_loader, optimizer, epoch):\n","    model.train()\n","    #pbar = tqdm(train_loader)\n","    train_loss = 0 \n","    train_correct = 0\n","    #scheduler.step()\n","    for batch_idx, (data, target) in enumerate(train_loader):\n","        data, target = data.to(device), target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = F.nll_loss(output, target)\n","        loss.backward()\n","        optimizer.step()\n","        #pbar.set_description(desc= f'loss={loss.item()} batch_id={batch_idx}')\n","        train_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n","        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","        train_correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","    train_loss /= len(test_loader.dataset)\n","    print('Epoch: {:.0f},LR: {}.\\nTrain set: train Average loss: {:.4f}, train_Accuracy: {}/{} ({:.4f}%)\\n'.format(\n","        epoch,optimizer.param_groups[0]['lr'],train_loss, train_correct, len(train_loader.dataset),\n","        100. * train_correct / len(train_loader.dataset)))\n","        \n","\n","def test(model, device, test_loader):\n","    model.eval()\n","    test_loss = 0\n","    correct = 0\n","    with torch.no_grad():\n","        for data, target in test_loader:\n","            data, target = data.to(device), target.to(device)\n","            output = model(data)\n","            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n","            pred_test = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n","            correct += pred_test.eq(target.view_as(pred_test)).sum().item()\n","\n","    test_loss /= len(test_loader.dataset)\n","\n","    \n","    print('Test set: test Average loss: {:.4f}, test Accuracy: {}/{} ({:.4f}%)\\n'.format(\n","        test_loss, correct, len(test_loader.dataset),\n","        100. * correct / len(test_loader.dataset)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MMWbLWO6FuHb","colab_type":"code","cellView":"both","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597861495068,"user_tz":-330,"elapsed":264324,"user":{"displayName":"pranav sai","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgUzfhrgEZs5GkRg3NiqbkWRHjeSfpEybcsDjhH=s64","userId":"17149605509345190981"}},"outputId":"edc6402d-0d73-4d7d-d18c-4d97c7aec97b"},"source":["model = Net().to(device)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","\n","for epoch in range(1, 16):\n","    train(model, device, train_loader, optimizer, epoch)\n","    test(model, device, test_loader)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch: 1,LR: 0.01.\n","Train set: train Average loss: 1.9995, train_Accuracy: 54690/60000 (91.1500%)\n","\n","Test set: test Average loss: 0.0658, test Accuracy: 9833/10000 (98.3300%)\n","\n","Epoch: 2,LR: 0.01.\n","Train set: train Average loss: 0.4477, train_Accuracy: 58746/60000 (97.9100%)\n","\n","Test set: test Average loss: 0.0390, test Accuracy: 9900/10000 (99.0000%)\n","\n","Epoch: 3,LR: 0.01.\n","Train set: train Average loss: 0.3436, train_Accuracy: 58989/60000 (98.3150%)\n","\n","Test set: test Average loss: 0.0330, test Accuracy: 9910/10000 (99.1000%)\n","\n","Epoch: 4,LR: 0.01.\n","Train set: train Average loss: 0.2999, train_Accuracy: 59108/60000 (98.5133%)\n","\n","Test set: test Average loss: 0.0316, test Accuracy: 9908/10000 (99.0800%)\n","\n","Epoch: 5,LR: 0.01.\n","Train set: train Average loss: 0.2596, train_Accuracy: 59219/60000 (98.6983%)\n","\n","Test set: test Average loss: 0.0286, test Accuracy: 9911/10000 (99.1100%)\n","\n","Epoch: 6,LR: 0.01.\n","Train set: train Average loss: 0.2358, train_Accuracy: 59309/60000 (98.8483%)\n","\n","Test set: test Average loss: 0.0266, test Accuracy: 9915/10000 (99.1500%)\n","\n","Epoch: 7,LR: 0.01.\n","Train set: train Average loss: 0.2257, train_Accuracy: 59290/60000 (98.8167%)\n","\n","Test set: test Average loss: 0.0302, test Accuracy: 9907/10000 (99.0700%)\n","\n","Epoch: 8,LR: 0.01.\n","Train set: train Average loss: 0.2024, train_Accuracy: 59393/60000 (98.9883%)\n","\n","Test set: test Average loss: 0.0227, test Accuracy: 9930/10000 (99.3000%)\n","\n","Epoch: 9,LR: 0.01.\n","Train set: train Average loss: 0.2031, train_Accuracy: 59365/60000 (98.9417%)\n","\n","Test set: test Average loss: 0.0220, test Accuracy: 9935/10000 (99.3500%)\n","\n","Epoch: 10,LR: 0.01.\n","Train set: train Average loss: 0.2034, train_Accuracy: 59363/60000 (98.9383%)\n","\n","Test set: test Average loss: 0.0230, test Accuracy: 9932/10000 (99.3200%)\n","\n","Epoch: 11,LR: 0.01.\n","Train set: train Average loss: 0.1791, train_Accuracy: 59434/60000 (99.0567%)\n","\n","Test set: test Average loss: 0.0221, test Accuracy: 9938/10000 (99.3800%)\n","\n","Epoch: 12,LR: 0.01.\n","Train set: train Average loss: 0.1637, train_Accuracy: 59475/60000 (99.1250%)\n","\n","Test set: test Average loss: 0.0214, test Accuracy: 9933/10000 (99.3300%)\n","\n","Epoch: 13,LR: 0.01.\n","Train set: train Average loss: 0.1692, train_Accuracy: 59488/60000 (99.1467%)\n","\n","Test set: test Average loss: 0.0229, test Accuracy: 9929/10000 (99.2900%)\n","\n","Epoch: 14,LR: 0.01.\n","Train set: train Average loss: 0.1671, train_Accuracy: 59453/60000 (99.0883%)\n","\n","Test set: test Average loss: 0.0183, test Accuracy: 9948/10000 (99.4800%)\n","\n","Epoch: 15,LR: 0.01.\n","Train set: train Average loss: 0.1533, train_Accuracy: 59539/60000 (99.2317%)\n","\n","Test set: test Average loss: 0.0212, test Accuracy: 9932/10000 (99.3200%)\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"So5uk4EkHW6R","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}